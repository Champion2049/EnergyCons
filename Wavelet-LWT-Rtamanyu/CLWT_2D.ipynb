{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5432ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f744c0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2D Data:\n",
      "[[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]\n",
      " [11 12 13 14 15]\n",
      " [16 17 18 19 20]\n",
      " [21 22 23 24 25]]\n",
      "Original 2D Data Shape: (5, 5)\n",
      "Decomposition Levels: 2\n",
      "------------------------------\n",
      "\n",
      "--- 2D Decomposition Results ---\n",
      "Final LL (Approximation) Sub-band:\n",
      "[[10.      3.125 ]\n",
      " [ 5.625   1.5625]]\n",
      "LH_L1 (Horizontal Detail Level 1):\n",
      "[[  1.    1.   -7.5]\n",
      " [  1.    1.  -17.5]\n",
      " [  0.5   0.5 -12.5]]\n",
      "HL_L1 (Vertical Detail Level 1):\n",
      "[[  5.    5.    2.5]\n",
      " [  5.    5.    2.5]\n",
      " [-21.5 -23.5 -12.5]]\n",
      "HH_L1 (Diagonal Detail Level 1):\n",
      "[[ 0.  0. -5.]\n",
      " [ 0.  0. -5.]\n",
      " [-1. -1. 25.]]\n",
      "LH_L2 (Horizontal Detail Level 2):\n",
      "[[ 2.    -6.25 ]\n",
      " [ 0.5   -3.125]]\n",
      "HL_L2 (Vertical Detail Level 2):\n",
      "[[ 10.      2.5  ]\n",
      " [-11.25   -3.125]]\n",
      "HH_L2 (Diagonal Detail Level 2):\n",
      "[[ 0.   -5.  ]\n",
      " [-1.    6.25]]\n",
      "------------------------------\n",
      "Saved all Haar LWT coefficients (in columns) to: haar_lwt_coeffs_2d/all_haar_lwt_coeffs_columns_nd.csv\n",
      "------------------------------\n",
      "\n",
      "--- 2D Reconstruction Results ---\n",
      "Reconstructed Data:\n",
      "[[ 1.  2.  3.  4.  5.]\n",
      " [ 6.  7.  8.  9. 10.]\n",
      " [11. 12. 13. 14. 15.]\n",
      " [16. 17. 18. 19. 20.]\n",
      " [21. 22. 23. 24. 25.]]\n",
      "Is 2D Reconstruction Accurate? True\n",
      "------------------------------\n",
      "\n",
      "--- Generalizing to N-Dimensional Data (Conceptual) ---\n",
      "For N-dimensional data beyond 2D (e.g., 3D volumes), the principle remains the same:\n",
      "You would recursively apply the 1D LWT along each dimension.\n",
      "For example, for 3D data, you would perform 1D LWT along axis 0, then axis 1, then axis 2.\n",
      "This would generate $2^N$ sub-bands at each level (e.g., 8 sub-bands for 3D: LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH).\n",
      "The complexity of managing these sub-bands and their reconstruction increases significantly.\n",
      "The current 2D implementation provides a solid foundation for understanding this generalization.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def haar_lwt_1d_decompose(data):\n",
    "    \"\"\"\n",
    "    Performs a single level 1D Haar Lifting Wavelet Transform decomposition.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The 1D input data array.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - approximation (np.ndarray): The approximation coefficients.\n",
    "            - detail (np.ndarray): The detail coefficients.\n",
    "            - original_len (int): The original length of the input data before padding.\n",
    "    \"\"\"\n",
    "    original_len = len(data)\n",
    "    \n",
    "    # Pad if length is odd to ensure even split\n",
    "    if original_len % 2 != 0:\n",
    "        padded_data = np.pad(data, (0, 1), 'constant', constant_values=0)\n",
    "    else:\n",
    "        padded_data = data\n",
    "\n",
    "    # Split: Separate into even and odd indexed samples\n",
    "    even = padded_data[::2]\n",
    "    odd = padded_data[1::2]\n",
    "\n",
    "    # Predict: Calculate detail coefficients (d_j = odd - even)\n",
    "    detail = odd - even\n",
    "\n",
    "    # Update: Calculate approximation coefficients (s_j = even + d_j / 2)\n",
    "    approximation = even + detail / 2\n",
    "\n",
    "    return approximation, detail, original_len\n",
    "\n",
    "def haar_lwt_1d_reconstruct(approximation, detail, original_len):\n",
    "    \"\"\"\n",
    "    Reconstructs a 1D signal from its single level Haar Lifting Wavelet Transform coefficients.\n",
    "\n",
    "    Args:\n",
    "        approximation (np.ndarray): The approximation coefficients.\n",
    "        detail (np.ndarray): The detail coefficients.\n",
    "        original_len (int): The original length of the signal before decomposition padding.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The reconstructed 1D signal.\n",
    "    \"\"\"\n",
    "    # Inverse Update\n",
    "    even = approximation - detail / 2\n",
    "\n",
    "    # Inverse Predict\n",
    "    odd = detail + even\n",
    "\n",
    "    # Merge: Combine the reconstructed even and odd parts\n",
    "    combined_padded_len = len(even) + len(odd)\n",
    "    reconstructed = np.empty(combined_padded_len, dtype=float)\n",
    "    reconstructed[::2] = even\n",
    "    reconstructed[1::2] = odd\n",
    "    \n",
    "    # Trim to original length\n",
    "    return reconstructed[:original_len]\n",
    "\n",
    "def haar_lwt_2d_decompose(data, level):\n",
    "    \"\"\"\n",
    "    Performs an N-level 2D Haar Lifting Wavelet Transform decomposition.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The 2D input data array (e.g., an image).\n",
    "        level (int): The number of decomposition levels to perform.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the final approximation sub-band and\n",
    "              all detail sub-bands (LH, HL, HH) for each level.\n",
    "              Keys will be 'LL', 'LH_L1', 'HL_L1', 'HH_L1', 'LH_L2', etc.\n",
    "              Each value is an np.ndarray.\n",
    "    \"\"\"\n",
    "    if data.ndim != 2:\n",
    "        raise ValueError(\"Input data must be 2-dimensional for 2D LWT.\")\n",
    "\n",
    "    current_approx = np.array(data, dtype=float)\n",
    "    coeffs_tree = {} # Stores all sub-bands\n",
    "\n",
    "    # Store original shape for reconstruction\n",
    "    coeffs_tree['original_shape'] = data.shape\n",
    "\n",
    "    for i in range(1, level + 1):\n",
    "        # --- Step 1: Apply 1D LWT along rows (axis=1) ---\n",
    "        # Initialize arrays to store row-wise approx and detail\n",
    "        rows_approx = np.empty((current_approx.shape[0], (current_approx.shape[1] + 1) // 2), dtype=float)\n",
    "        rows_detail = np.empty((current_approx.shape[0], (current_approx.shape[1] + 1) // 2), dtype=float)\n",
    "        \n",
    "        # Store original row lengths for reconstruction\n",
    "        original_row_lengths = []\n",
    "\n",
    "        for r in range(current_approx.shape[0]):\n",
    "            row_approx, row_detail, original_len = haar_lwt_1d_decompose(current_approx[r, :])\n",
    "            rows_approx[r, :len(row_approx)] = row_approx # Ensure correct assignment for padded/smaller arrays\n",
    "            rows_detail[r, :len(row_detail)] = row_detail\n",
    "            original_row_lengths.append(original_len)\n",
    "        \n",
    "        # Store original row lengths for this level\n",
    "        coeffs_tree[f'original_row_lengths_L{i}'] = original_row_lengths\n",
    "\n",
    "        # --- Step 2: Apply 1D LWT along columns (axis=0) to rows_approx and rows_detail ---\n",
    "        # Transpose to apply 1D LWT along columns\n",
    "        cols_approx_from_rows_approx = np.empty(((rows_approx.shape[0] + 1) // 2, rows_approx.shape[1]), dtype=float)\n",
    "        cols_detail_from_rows_approx = np.empty(((rows_approx.shape[0] + 1) // 2, rows_approx.shape[1]), dtype=float)\n",
    "        \n",
    "        cols_approx_from_rows_detail = np.empty(((rows_detail.shape[0] + 1) // 2, rows_detail.shape[1]), dtype=float)\n",
    "        cols_detail_from_rows_detail = np.empty(((rows_detail.shape[0] + 1) // 2, rows_detail.shape[1]), dtype=float)\n",
    "\n",
    "        # Store original column lengths for reconstruction\n",
    "        original_col_lengths_approx = []\n",
    "        original_col_lengths_detail = []\n",
    "\n",
    "        for c in range(rows_approx.shape[1]):\n",
    "            # For LL and HL (from rows_approx)\n",
    "            col_approx_ll, col_detail_hl, original_len_col_approx = haar_lwt_1d_decompose(rows_approx[:, c])\n",
    "            cols_approx_from_rows_approx[:len(col_approx_ll), c] = col_approx_ll\n",
    "            cols_detail_from_rows_approx[:len(col_detail_hl), c] = col_detail_hl\n",
    "            original_col_lengths_approx.append(original_len_col_approx)\n",
    "\n",
    "            # For LH and HH (from rows_detail)\n",
    "            col_approx_lh, col_detail_hh, original_len_col_detail = haar_lwt_1d_decompose(rows_detail[:, c])\n",
    "            cols_approx_from_rows_detail[:len(col_approx_lh), c] = col_approx_lh\n",
    "            cols_detail_from_rows_detail[:len(col_detail_hh), c] = col_detail_hh\n",
    "            original_col_lengths_detail.append(original_len_col_detail)\n",
    "        \n",
    "        # Store original column lengths for this level\n",
    "        coeffs_tree[f'original_col_lengths_approx_L{i}'] = original_col_lengths_approx\n",
    "        coeffs_tree[f'original_col_lengths_detail_L{i}'] = original_col_lengths_detail\n",
    "\n",
    "        # Assign sub-bands for the current level\n",
    "        # LL is the approximation for the next level\n",
    "        current_approx = cols_approx_from_rows_approx\n",
    "        coeffs_tree[f'LH_L{i}'] = cols_approx_from_rows_detail # Low-pass on rows, High-pass on columns\n",
    "        coeffs_tree[f'HL_L{i}'] = cols_detail_from_rows_approx # High-pass on rows, Low-pass on columns\n",
    "        coeffs_tree[f'HH_L{i}'] = cols_detail_from_rows_detail # High-pass on rows, High-pass on columns\n",
    "        \n",
    "        # Break if the approximation sub-band becomes too small for further decomposition\n",
    "        if current_approx.shape[0] < 2 or current_approx.shape[1] < 2:\n",
    "            print(f\"Warning: Stopped 2D decomposition at level {i} because approximation sub-band became too small: {current_approx.shape}\")\n",
    "            break\n",
    "    \n",
    "    # The final approximation sub-band\n",
    "    coeffs_tree['LL'] = current_approx\n",
    "    return coeffs_tree\n",
    "\n",
    "def haar_lwt_2d_reconstruct(coeffs_tree):\n",
    "    \"\"\"\n",
    "    Reconstructs a 2D signal from its N-level Haar Lifting Wavelet Transform coefficients.\n",
    "\n",
    "    Args:\n",
    "        coeffs_tree (dict): A dictionary containing the final approximation sub-band\n",
    "                            and all detail sub-bands (LH, HL, HH) for each level,\n",
    "                            as returned by `haar_lwt_2d_decompose`.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The reconstructed 2D signal.\n",
    "    \"\"\"\n",
    "    current_reconstruction = coeffs_tree['LL']\n",
    "    original_full_shape = coeffs_tree['original_shape']\n",
    "    \n",
    "    # Determine the number of levels from the keys in coeffs_tree\n",
    "    levels = 0\n",
    "    for key in coeffs_tree:\n",
    "        if key.startswith('LH_L'):\n",
    "            levels = max(levels, int(key.split('_L')[1]))\n",
    "    \n",
    "    # Reconstruct level by level, from coarsest to finest\n",
    "    for i in range(levels, 0, -1):\n",
    "        lh = coeffs_tree[f'LH_L{i}']\n",
    "        hl = coeffs_tree[f'HL_L{i}']\n",
    "        hh = coeffs_tree[f'HH_L{i}']\n",
    "        \n",
    "        original_col_lengths_approx = coeffs_tree[f'original_col_lengths_approx_L{i}']\n",
    "        original_col_lengths_detail = coeffs_tree[f'original_col_lengths_detail_L{i}']\n",
    "        original_row_lengths = coeffs_tree[f'original_row_lengths_L{i}']\n",
    "\n",
    "        # --- Inverse Step 2: Reconstruct columns (axis=0) ---\n",
    "        # Reconstruct rows_approx from current_reconstruction (LL from previous level) and HL\n",
    "        reconstructed_rows_approx = np.empty((max(original_col_lengths_approx), current_reconstruction.shape[1]), dtype=float)\n",
    "        for c in range(current_reconstruction.shape[1]):\n",
    "            reconstructed_rows_approx[:, c] = haar_lwt_1d_reconstruct(\n",
    "                current_reconstruction[:, c], hl[:, c], original_col_lengths_approx[c]\n",
    "            )\n",
    "        \n",
    "        # Reconstruct rows_detail from LH and HH\n",
    "        reconstructed_rows_detail = np.empty((max(original_col_lengths_detail), lh.shape[1]), dtype=float)\n",
    "        for c in range(lh.shape[1]):\n",
    "            reconstructed_rows_detail[:, c] = haar_lwt_1d_reconstruct(\n",
    "                lh[:, c], hh[:, c], original_col_lengths_detail[c]\n",
    "            )\n",
    "\n",
    "        # --- Inverse Step 1: Reconstruct rows (axis=1) ---\n",
    "        # Merge reconstructed_rows_approx and reconstructed_rows_detail\n",
    "        # This is the tricky part: we need to merge the approx and detail *matrices*\n",
    "        # The 1D reconstruct function expects 1D arrays.\n",
    "        # We need to apply the 1D reconstruct function row-wise to combine\n",
    "        # `reconstructed_rows_approx` (which is the 'even' part conceptually)\n",
    "        # and `reconstructed_rows_detail` (which is the 'detail' part conceptually).\n",
    "        \n",
    "        # The shape of the output of this step should be the original shape of the\n",
    "        # data at this level before the row-wise decomposition.\n",
    "        \n",
    "        # The number of rows in reconstructed_rows_approx and reconstructed_rows_detail should be the same\n",
    "        # This will be the height of the image at this level.\n",
    "        reconstructed_current_level = np.empty((reconstructed_rows_approx.shape[0], max(original_row_lengths)), dtype=float)\n",
    "\n",
    "        for r in range(reconstructed_rows_approx.shape[0]):\n",
    "            # The 'approximation' input to 1D reconstruct is the 'even' part (reconstructed_rows_approx[r, :])\n",
    "            # The 'detail' input to 1D reconstruct is the 'odd - even' part (reconstructed_rows_detail[r, :])\n",
    "            # This is where the inverse lifting steps (even = s - d/2, odd = d + even) are applied.\n",
    "            \n",
    "            # The `haar_lwt_1d_reconstruct` already handles the merging logic.\n",
    "            # We need to treat `reconstructed_rows_approx` as the approximation part\n",
    "            # and `reconstructed_rows_detail` as the detail part for the row-wise reconstruction.\n",
    "            \n",
    "            # This is a conceptual mapping. The `haar_lwt_1d_reconstruct` takes (approx, detail, original_len).\n",
    "            # Here, `reconstructed_rows_approx` acts as the 's_j' and `reconstructed_rows_detail` acts as 'd_j'.\n",
    "            \n",
    "            # The `haar_lwt_1d_reconstruct` expects the *approximation* and *detail* coefficients\n",
    "            # from a *single level* 1D decomposition.\n",
    "            # So, we need to apply the inverse of the row-wise transform.\n",
    "            # This means taking the `reconstructed_rows_approx` (which is effectively LL/HL)\n",
    "            # and `reconstructed_rows_detail` (which is effectively LH/HH)\n",
    "            # and combining them row by row.\n",
    "\n",
    "            # The `haar_lwt_1d_reconstruct` function needs the approximation and detail coefficients\n",
    "            # that were *directly* produced by a 1D decomposition.\n",
    "            # The `reconstructed_rows_approx` and `reconstructed_rows_detail` are already the result\n",
    "            # of column-wise reconstruction. They are not the direct 'approximation' and 'detail'\n",
    "            # for the row-wise inverse.\n",
    "\n",
    "            # Let's rethink the 2D reconstruction.\n",
    "            # After column-wise reconstruction, we have `reconstructed_rows_approx` (which is the A part of the row-wise transform)\n",
    "            # and `reconstructed_rows_detail` (which is the D part of the row-wise transform).\n",
    "            # We need to merge these two 2D arrays back into the original 2D array for this level.\n",
    "\n",
    "            # The `haar_lwt_1d_reconstruct` takes `approximation`, `detail`, `original_len`.\n",
    "            # We need to apply this row by row.\n",
    "            # `reconstructed_rows_approx[r, :]` is the approximation part for row `r`.\n",
    "            # `reconstructed_rows_detail[r, :]` is the detail part for row `r`.\n",
    "            reconstructed_current_level[r, :] = haar_lwt_1d_reconstruct(\n",
    "                reconstructed_rows_approx[r, :],\n",
    "                reconstructed_rows_detail[r, :],\n",
    "                original_row_lengths[r]\n",
    "            )\n",
    "        \n",
    "        current_reconstruction = reconstructed_current_level\n",
    "\n",
    "    # The final reconstruction needs to be trimmed to the original full shape\n",
    "    return current_reconstruction[:original_full_shape[0], :original_full_shape[1]]\n",
    "\n",
    "\n",
    "def save_coefficients_to_files(output_dir, coeffs_tree):\n",
    "    \"\"\"\n",
    "    Saves all Haar LWT coefficients (approximation and all detail levels) into a single .csv file,\n",
    "    with each coefficient type in its own column.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): The directory where the coefficient file will be saved.\n",
    "                          If the directory does not exist, it will be created.\n",
    "        coeffs_tree (dict): A dictionary containing the final approximation sub-band\n",
    "                            and all detail sub-bands (LH, HL, HH) for each level,\n",
    "                            as returned by `haar_lwt_2d_decompose`.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare a list of all flattened coefficient arrays and their headers\n",
    "    all_coeff_arrays_flat = []\n",
    "    headers = []\n",
    "\n",
    "    # Add the final approximation (LL)\n",
    "    if 'LL' in coeffs_tree:\n",
    "        all_coeff_arrays_flat.append(coeffs_tree['LL'].flatten())\n",
    "        headers.append('LL_Approximation')\n",
    "\n",
    "    # Add detail coefficients for each level\n",
    "    levels = 0\n",
    "    for key in coeffs_tree:\n",
    "        if key.startswith('LH_L'):\n",
    "            levels = max(levels, int(key.split('_L')[1]))\n",
    "\n",
    "    for i in range(1, levels + 1):\n",
    "        if f'LH_L{i}' in coeffs_tree:\n",
    "            all_coeff_arrays_flat.append(coeffs_tree[f'LH_L{i}'].flatten())\n",
    "            headers.append(f'LH_L{i}')\n",
    "        if f'HL_L{i}' in coeffs_tree:\n",
    "            all_coeff_arrays_flat.append(coeffs_tree[f'HL_L{i}'].flatten())\n",
    "            headers.append(f'HL_L{i}')\n",
    "        if f'HH_L{i}' in coeffs_tree:\n",
    "            all_coeff_arrays_flat.append(coeffs_tree[f'HH_L{i}'].flatten())\n",
    "            headers.append(f'HH_L{i}')\n",
    "\n",
    "    # Find the maximum length among all flattened coefficient arrays for consistent column size\n",
    "    max_len = 0\n",
    "    if all_coeff_arrays_flat:\n",
    "        max_len = max(len(arr) for arr in all_coeff_arrays_flat)\n",
    "\n",
    "    # Pad shorter arrays with NaN to match the maximum length\n",
    "    padded_coeff_arrays = []\n",
    "    for arr in all_coeff_arrays_flat:\n",
    "        if len(arr) < max_len:\n",
    "            padded_arr = np.pad(arr, (0, max_len - len(arr)), 'constant', constant_values=np.nan)\n",
    "        else:\n",
    "            padded_arr = arr\n",
    "        padded_coeff_arrays.append(padded_arr)\n",
    "\n",
    "    # Stack the padded arrays horizontally to form a 2D array (columns)\n",
    "    if padded_coeff_arrays:\n",
    "        combined_coeffs_2d = np.column_stack(padded_coeff_arrays)\n",
    "    else:\n",
    "        combined_coeffs_2d = np.array([[]]) # Handle case where no coefficients are generated\n",
    "\n",
    "    # Create the header string for the CSV file\n",
    "    header_str = ','.join(headers)\n",
    "\n",
    "    # Save the combined coefficients to a single CSV file\n",
    "    combined_filepath = os.path.join(output_dir, 'all_haar_lwt_coeffs_columns_nd.csv')\n",
    "    np.savetxt(combined_filepath, combined_coeffs_2d, delimiter=',', header=header_str, comments='')\n",
    "    print(f\"Saved all Haar LWT coefficients (in columns) to: {combined_filepath}\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Define sample 2D data (e.g., a small image)\n",
    "    # Example with even dimensions\n",
    "    data_2d_even = np.array([\n",
    "        [10, 20, 30, 40],\n",
    "        [50, 60, 70, 80],\n",
    "        [90, 100, 110, 120],\n",
    "        [130, 140, 150, 160]\n",
    "    ])\n",
    "    # Example with odd dimensions\n",
    "    data_2d_odd = np.array([\n",
    "        [1, 2, 3, 4, 5],\n",
    "        [6, 7, 8, 9, 10],\n",
    "        [11, 12, 13, 14, 15],\n",
    "        [16, 17, 18, 19, 20],\n",
    "        [21, 22, 23, 24, 25]\n",
    "    ])\n",
    "    \n",
    "    # Choose which data to use for demonstration\n",
    "    input_data_2d = data_2d_odd\n",
    "    n_levels_2d = 2 # Number of decomposition levels\n",
    "\n",
    "    print(f\"Original 2D Data:\\n{input_data_2d}\")\n",
    "    print(f\"Original 2D Data Shape: {input_data_2d.shape}\")\n",
    "    print(f\"Decomposition Levels: {n_levels_2d}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 2. Perform N-level 2D Haar LWT decomposition\n",
    "    coeffs_2d = haar_lwt_2d_decompose(input_data_2d, n_levels_2d)\n",
    "\n",
    "    print(\"\\n--- 2D Decomposition Results ---\")\n",
    "    print(f\"Final LL (Approximation) Sub-band:\\n{coeffs_2d['LL']}\")\n",
    "    for i in range(1, n_levels_2d + 1):\n",
    "        if f'LH_L{i}' in coeffs_2d:\n",
    "            print(f\"LH_L{i} (Horizontal Detail Level {i}):\\n{coeffs_2d[f'LH_L{i}']}\")\n",
    "        if f'HL_L{i}' in coeffs_2d:\n",
    "            print(f\"HL_L{i} (Vertical Detail Level {i}):\\n{coeffs_2d[f'HL_L{i}']}\")\n",
    "        if f'HH_L{i}' in coeffs_2d:\n",
    "            print(f\"HH_L{i} (Diagonal Detail Level {i}):\\n{coeffs_2d[f'HH_L{i}']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 3. Save coefficients to files\n",
    "    output_directory_2d = 'haar_lwt_coeffs_2d'\n",
    "    save_coefficients_to_files(output_directory_2d, coeffs_2d)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 4. Perform 2D reconstruction\n",
    "    reconstructed_data_2d = haar_lwt_2d_reconstruct(coeffs_2d)\n",
    "\n",
    "    print(\"\\n--- 2D Reconstruction Results ---\")\n",
    "    print(f\"Reconstructed Data:\\n{reconstructed_data_2d}\")\n",
    "    \n",
    "    # 5. Verify 2D reconstruction accuracy\n",
    "    is_reconstruction_accurate_2d = np.allclose(input_data_2d, reconstructed_data_2d)\n",
    "    print(f\"Is 2D Reconstruction Accurate? {is_reconstruction_accurate_2d}\")\n",
    "    if not is_reconstruction_accurate_2d:\n",
    "        print(f\"Difference:\\n{input_data_2d - reconstructed_data_2d}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(\"\\n--- Generalizing to N-Dimensional Data (Conceptual) ---\")\n",
    "    print(\"For N-dimensional data beyond 2D (e.g., 3D volumes), the principle remains the same:\")\n",
    "    print(\"You would recursively apply the 1D LWT along each dimension.\")\n",
    "    print(\"For example, for 3D data, you would perform 1D LWT along axis 0, then axis 1, then axis 2.\")\n",
    "    print(\"This would generate $2^N$ sub-bands at each level (e.g., 8 sub-bands for 3D: LLL, LLH, LHL, LHH, HLL, HLH, HHL, HHH).\")\n",
    "    print(\"The complexity of managing these sub-bands and their reconstruction increases significantly.\")\n",
    "    print(\"The current 2D implementation provides a solid foundation for understanding this generalization.\")\n",
    "    print(\"-\" * 30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Normal_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
