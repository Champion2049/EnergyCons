{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c309479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f8388c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 1D Data: [7.343 8.013 8.293 ... 9.517 9.46  9.309]\n",
      "Decomposition Levels: 2\n",
      "------------------------------\n",
      "\n",
      "--- Decomposition Results ---\n",
      "Final Approximation Coefficients (Reduced Values): [8.09975 9.22075 9.9     ... 8.84425 9.15425 9.42825]\n",
      "Detail Coefficients per Level (from finest to coarsest detail):\n",
      "  Level 1 Detail: [ 0.67   0.457  0.235 ...  0.106  0.09  -0.151] (Original length at this level: 4176)\n",
      "  Level 2 Detail: [ 0.8435  0.0845  0.647  ...  0.2315  0.1375 -0.0875] (Original length at this level: 2088)\n",
      "------------------------------\n",
      "Saved all Haar LWT coefficients (in columns) to: haar_lwt_coeffs/all_haar_lwt_coeffs_columns.csv\n",
      "------------------------------\n",
      "\n",
      "--- Reconstruction Results ---\n",
      "Reconstructed Data: [7.343 8.013 8.293 ... 9.517 9.46  9.309]\n",
      "Is Reconstruction Accurate? True\n",
      "------------------------------\n",
      "\n",
      "--- Extending to N-Dimensional Data ---\n",
      "For N-dimensional data (e.g., 2D images), the 1D transform is typically applied\n",
      "iteratively along each dimension. For example, for a 2D image:\n",
      "1. Apply 1D LWT to each row.\n",
      "2. Apply 1D LWT to each column of the resulting coefficients.\n",
      "This process can be generalized for 3D or higher dimensions.\n",
      "You would need to reshape your N-dimensional data into 1D segments (rows, columns, slices),\n",
      "apply this 1D function, and then reshape back. The reconstruction would follow the inverse steps.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def haar_lwt_1d_decompose(data, level):\n",
    "    \"\"\"\n",
    "    Performs a 1D Haar Lifting Wavelet Transform decomposition for a given number of levels.\n",
    "\n",
    "    The lifting scheme for Haar wavelet involves:\n",
    "    1. Split: Separate the signal into even and odd indexed samples.\n",
    "    2. Predict: Calculate detail coefficients (high-frequency) by subtracting\n",
    "       the even samples from the odd samples.\n",
    "    3. Update: Calculate approximation coefficients (low-frequency) by adding\n",
    "       half of the detail coefficients to the even samples.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The 1D input data array.\n",
    "        level (int): The number of decomposition levels to perform.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - final_approximation_coeffs (np.ndarray): The approximation coefficients\n",
    "              at the highest decomposition level. This represents the \"reduced values\".\n",
    "            - detail_coeffs_info_list (list): A list of tuples, where each tuple contains:\n",
    "                (detail_coeffs (np.ndarray), original_length_at_this_level (int))\n",
    "              The list is ordered from the lowest decomposition level (finest details)\n",
    "              to the highest decomposition level (coarsest details).\n",
    "              The `original_length_at_this_level` is crucial for accurate reconstruction\n",
    "              when dealing with signals that had odd lengths at certain stages.\n",
    "    \"\"\"\n",
    "    # Ensure data is a float numpy array for calculations\n",
    "    current_coeffs = np.array(data, dtype=float)\n",
    "    \n",
    "    # List to store detail coefficients and their original lengths at each level\n",
    "    detail_coeffs_info_list = [] \n",
    "\n",
    "    # Perform decomposition for the specified number of levels\n",
    "    for i in range(level):\n",
    "        # Store the original length of the signal at the current level before any padding\n",
    "        original_len_at_this_level = len(current_coeffs)\n",
    "\n",
    "        # Pad the current coefficients if their length is odd.\n",
    "        # This ensures that 'even' and 'odd' parts have compatible lengths.\n",
    "        if original_len_at_this_level % 2 != 0:\n",
    "            # Pad with a single zero at the end\n",
    "            padded_coeffs = np.pad(current_coeffs, (0, 1), 'constant')\n",
    "        else:\n",
    "            padded_coeffs = current_coeffs\n",
    "\n",
    "        # Step 1: Split - Separate into even and odd indexed samples\n",
    "        even = padded_coeffs[::2]\n",
    "        odd = padded_coeffs[1::2]\n",
    "\n",
    "        # Step 2: Predict - Calculate detail coefficients (d_j = odd - even)\n",
    "        detail = odd - even\n",
    "\n",
    "        # Step 3: Update - Calculate approximation coefficients (s_j = even + d_j / 2)\n",
    "        approximation = even + detail / 2\n",
    "\n",
    "        # Store the detail coefficients along with the original length of the signal\n",
    "        # at this level (before padding), which is needed for accurate reconstruction.\n",
    "        detail_coeffs_info_list.append((detail, original_len_at_this_level))\n",
    "        \n",
    "        # The approximation coefficients become the input for the next decomposition level\n",
    "        current_coeffs = approximation\n",
    "\n",
    "        # Stop decomposition if the approximation coefficients become too small (e.g., single element),\n",
    "        # as further decomposition is not meaningful.\n",
    "        if len(current_coeffs) < 2:\n",
    "            print(f\"Warning: Stopped decomposition at level {i+1} because signal length became too small.\")\n",
    "            break\n",
    "\n",
    "    # The final `current_coeffs` are the approximation coefficients at the highest level\n",
    "    return current_coeffs, detail_coeffs_info_list\n",
    "\n",
    "def haar_lwt_1d_reconstruct(final_approximation_coeffs, detail_coeffs_info_list):\n",
    "    \"\"\"\n",
    "    Reconstructs a 1D signal from its Haar Lifting Wavelet Transform coefficients.\n",
    "\n",
    "    The inverse lifting scheme for Haar wavelet involves:\n",
    "    1. Inverse Update: Revert the update step (even = s_j - d_j / 2).\n",
    "    2. Inverse Predict: Revert the predict step (odd = d_j + even).\n",
    "    3. Merge: Combine the reconstructed even and odd parts to form the signal\n",
    "       at the previous level, trimming any padding if necessary.\n",
    "\n",
    "    Args:\n",
    "        final_approximation_coeffs (np.ndarray): The approximation coefficients\n",
    "                                                at the highest decomposition level.\n",
    "        detail_coeffs_info_list (list): A list of tuples, where each tuple contains:\n",
    "                                        (detail_coeffs (np.ndarray), original_length_at_this_level (int))\n",
    "                                        This list should be in the order from lowest\n",
    "                                        to highest level of detail (as returned by decompose).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The reconstructed 1D signal, which should ideally be identical\n",
    "                    to the original input data.\n",
    "    \"\"\"\n",
    "    # Start reconstruction from the highest level approximation coefficients\n",
    "    reconstructed_coeffs = np.array(final_approximation_coeffs, dtype=float)\n",
    "\n",
    "    # Reconstruct level by level, processing detail coefficients from highest to lowest level\n",
    "    # (i.e., in reverse order of how they were generated during decomposition)\n",
    "    for detail_info in reversed(detail_coeffs_info_list):\n",
    "        detail, original_len_at_this_level = detail_info\n",
    "\n",
    "        # Step 1: Inverse Update\n",
    "        # Revert the approximation calculation to get the 'even' part back\n",
    "        even = reconstructed_coeffs - detail / 2\n",
    "\n",
    "        # Step 2: Inverse Predict\n",
    "        # Revert the detail calculation to get the 'odd' part back\n",
    "        odd = detail + even\n",
    "\n",
    "        # Step 3: Merge - Combine the even and odd parts\n",
    "        # The combined length will be the length of the signal at the previous level,\n",
    "        # potentially including padding if the original signal at that level was odd.\n",
    "        combined_padded_len = len(even) + len(odd)\n",
    "        temp_reconstructed = np.empty(combined_padded_len, dtype=float)\n",
    "        \n",
    "        # Place even samples at even indices and odd samples at odd indices\n",
    "        temp_reconstructed[::2] = even\n",
    "        temp_reconstructed[1::2] = odd\n",
    "        \n",
    "        # Trim the reconstructed signal to its original length at this level.\n",
    "        # This removes any padding that was added during decomposition.\n",
    "        reconstructed_coeffs = temp_reconstructed[:original_len_at_this_level]\n",
    "\n",
    "    return reconstructed_coeffs\n",
    "\n",
    "def save_coefficients_to_files(output_dir, final_approximation, detail_coeffs_info_list):\n",
    "    \"\"\"\n",
    "    Saves all Haar LWT coefficients (approximation and all detail levels) into a single .csv file,\n",
    "    with each coefficient type in its own column.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): The directory where the coefficient file will be saved.\n",
    "                          If the directory does not exist, it will be created.\n",
    "        final_approximation (np.ndarray): The final approximation coefficients\n",
    "                                          (representing the reduced values).\n",
    "        detail_coeffs_info_list (list): A list of tuples (detail_coeffs, original_length)\n",
    "                                        for each decomposition level.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare a list of all coefficient arrays to be combined into columns\n",
    "    all_coeff_arrays = [final_approximation]\n",
    "    headers = ['Approximation']\n",
    "\n",
    "    for i, (detail, _) in enumerate(detail_coeffs_info_list):\n",
    "        all_coeff_arrays.append(detail)\n",
    "        headers.append(f'Detail_Level_{i+1}')\n",
    "\n",
    "    # Find the maximum length among all coefficient arrays for consistent column size\n",
    "    max_len = 0\n",
    "    if all_coeff_arrays: # Ensure there's at least one array to check length\n",
    "        max_len = max(len(arr) for arr in all_coeff_arrays)\n",
    "\n",
    "    # Pad shorter arrays with NaN to match the maximum length\n",
    "    # This ensures all columns have the same number of rows\n",
    "    padded_coeff_arrays = []\n",
    "    for arr in all_coeff_arrays:\n",
    "        if len(arr) < max_len:\n",
    "            padded_arr = np.pad(arr, (0, max_len - len(arr)), 'constant', constant_values=np.nan)\n",
    "        else:\n",
    "            padded_arr = arr\n",
    "        padded_coeff_arrays.append(padded_arr)\n",
    "\n",
    "    # Stack the padded arrays horizontally to form a 2D array (columns)\n",
    "    if padded_coeff_arrays:\n",
    "        combined_coeffs_2d = np.column_stack(padded_coeff_arrays)\n",
    "    else:\n",
    "        combined_coeffs_2d = np.array([[]]) # Handle case where no coefficients are generated\n",
    "\n",
    "    # Create the header string for the CSV file\n",
    "    header_str = ','.join(headers)\n",
    "\n",
    "    # Save the combined coefficients to a single CSV file\n",
    "    combined_filepath = os.path.join(output_dir, 'all_haar_lwt_coeffs_columns.csv')\n",
    "    np.savetxt(combined_filepath, combined_coeffs_2d, delimiter=',', header=header_str, comments='')\n",
    "    print(f\"Saved all Haar LWT coefficients (in columns) to: {combined_filepath}\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Define sample 1D data\n",
    "    # Example with even length\n",
    "    data_even = np.array([10, 20, 30, 40, 50, 60, 70, 80])\n",
    "    # Example with odd length\n",
    "    data_odd = np.array([1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "    df = pd.read_csv('Data_August_Renewable.csv')\n",
    "    data = df['Speed'].values\n",
    "    \n",
    "    # Choose which data to use for demonstration\n",
    "    # input_data = data_odd \n",
    "    input_data = data\n",
    "    n_levels = 2 # Number of decomposition levels\n",
    "\n",
    "    print(f\"Original 1D Data: {input_data}\")\n",
    "    print(f\"Decomposition Levels: {n_levels}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 2. Perform N-level Haar LWT decomposition\n",
    "    final_approx, detail_coeffs_info = haar_lwt_1d_decompose(input_data, n_levels)\n",
    "\n",
    "    print(\"\\n--- Decomposition Results ---\")\n",
    "    print(f\"Final Approximation Coefficients (Reduced Values): {final_approx}\")\n",
    "    print(\"Detail Coefficients per Level (from finest to coarsest detail):\")\n",
    "    for i, (detail, original_len) in enumerate(detail_coeffs_info):\n",
    "        print(f\"  Level {i+1} Detail: {detail} (Original length at this level: {original_len})\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 3. Save coefficients to files\n",
    "    output_directory = 'haar_lwt_coeffs'\n",
    "    save_coefficients_to_files(output_directory, final_approx, detail_coeffs_info)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 4. Perform reconstruction\n",
    "    reconstructed_data = haar_lwt_1d_reconstruct(final_approx, detail_coeffs_info)\n",
    "\n",
    "    print(\"\\n--- Reconstruction Results ---\")\n",
    "    print(f\"Reconstructed Data: {reconstructed_data}\")\n",
    "    \n",
    "    # 5. Verify reconstruction accuracy\n",
    "    # Use np.isclose for floating-point comparisons\n",
    "    is_reconstruction_accurate = np.allclose(input_data, reconstructed_data)\n",
    "    print(f\"Is Reconstruction Accurate? {is_reconstruction_accurate}\")\n",
    "    if not is_reconstruction_accurate:\n",
    "        print(f\"Difference: {input_data - reconstructed_data}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- How to extend to N-dimensional data (Conceptual) ---\n",
    "    print(\"\\n--- Extending to N-Dimensional Data ---\")\n",
    "    print(\"For N-dimensional data (e.g., 2D images), the 1D transform is typically applied\")\n",
    "    print(\"iteratively along each dimension. For example, for a 2D image:\")\n",
    "    print(\"1. Apply 1D LWT to each row.\")\n",
    "    print(\"2. Apply 1D LWT to each column of the resulting coefficients.\")\n",
    "    print(\"This process can be generalized for 3D or higher dimensions.\")\n",
    "    print(\"You would need to reshape your N-dimensional data into 1D segments (rows, columns, slices),\")\n",
    "    print(\"apply this 1D function, and then reshape back. The reconstruction would follow the inverse steps.\")\n",
    "    print(\"-\" * 30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
